{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-0de483247578>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-0de483247578>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    <span style=\"color:green\"></span>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# This presentation is a Jupyter notebook! (thanks to https://github.com/damianavila/RISE )\n",
    "<span style=\"color:green\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Physics analysis as a differentiable program\n",
    "Lukas Heinrich, **Nathan Simpson** <- me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<figure style=\"display:flex; padding-right: 20px; padding left: 20px;\">\n",
    "    <img src=\"assets/cern.jpg\" width=\"160\" />\n",
    "    <img src=\"assets/lu.png\" width=\"160\" /> \n",
    "    <img src=\"assets/gradhep.png\" width=\"160\" />\n",
    "    <img src=\"assets/insights.jpg\" width=\"230\" /> \n",
    "    <img src=\"assets/eu.png\" width=\"230\" />  \n",
    "<figure/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"assets/anaflow.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**A typical analysis:** \n",
    "\n",
    "<span style=\"color:blue\">data</span>  →      <span style=\"color:maroon\">cutflow</span> → <span style=\"color:green\">observable</span> → <span style=\"color:lightblue\">model</span> → <span style=\"color:teal\">test statistic</span> → <span style=\"color:orange\">hypothesis test</span> → <span style=\"color:darkyellow\">*limits* (p-values)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"assets/free.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<span style=\"color:orange\">**Free parameters:**</span> cut values, parameters of a multivariate observable\n",
    "\n",
    "*Can we optimize them?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Sure we can!** \n",
    "\n",
    "Here's what we might do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *Cuts:*  grid search of <span style=\"color:orange\">cut positions</span>\n",
    "    - Objective: <span style=\"color:blue\">maximize approximate median significance $Z_{A}^{\\left(\\sigma_{b}=0\\right)}=\\sqrt{2((s+b) \\ln (1+s / b)-s)}$</span>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *Observable (e.g. neural network):* train parameters <span style=\"color:orange\">$\\phi$</span> via gradient descent\n",
    "    - Objective: <span style=\"color:blue\">minimize binary cross entropy (signal vs background)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why do the methods and objectives differ if they're in the same pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**They don't have to!**\n",
    "\n",
    "Sigmoid cut optimized wrt $Z_A$ using gradient descent: ([notebook from Alex Held](https://mybinder.org/v2/gh/alexander-held/differentiable-analysis-example/master?filepath=Significance_optimization.ipynb))\n",
    "\n",
    "<img src=\"assets/cut.gif\" width=\"50%\"/>\n",
    "\n",
    "Using $1/Z_A$ as a loss function: https://arxiv.org/abs/1806.00322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One could then imagine a *jointly-optimized pipeline* with respect to a single objective, trained using <span style=\"color:purple\">gradient descent</span>.\n",
    "\n",
    "This raises an important question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- <span style=\"color:green\">**What should that objective be?**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Picking the right objective\n",
    "\n",
    "Insights from Sec 3.1, [Deep Learning and its Application to LHC Physics](https://arxiv.org/abs/1806.11484):\n",
    "\n",
    "> <span style=\"color:blue\">\"tools are often optimized for performance on a particular task that is several steps removed from the ultimate physical goal of searching for a new particle or testing a new\n",
    "physical theory\"</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Picking the right objective\n",
    "\n",
    "Insights from Sec 3.1, [Deep Learning and its Application to LHC Physics](https://arxiv.org/abs/1806.11484):\n",
    "\n",
    "> <span style=\"color:red\">\"a new classifier may have a better false-positive rate than\n",
    "a baseline algorithm, yet simultaneously be more susceptible to systematic mismodeling\n",
    "between the simulation and the real data\"</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "We need to account for the <span style=\"color:teal\">*full profile likelihood*</span> in order to be robust to systematic variations of nusiance parameters.\n",
    "\n",
    "Let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"assets/anaflowgrad.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can optimize our analysis *end-to-end* with respect to the true significance... if we can <span style=\"color:purple\">evaluate its gradient</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One could then imagine a *jointly-optimized pipeline* with respect to a single objective, trained using <span style=\"color:purple\">gradient descent</span>.\n",
    "\n",
    "This raises ~an~ *two* important question*s*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:gray\">What should that objective be?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:purple\">**Can we evaluate its gradient?**</span>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "rise": {
   "scroll": true,
   "theme": "simple",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
