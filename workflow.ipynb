{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "from jax.experimental import stax\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as npr\n",
    "import pyhf\n",
    "from pyhf.simplemodels import correlated_background\n",
    "from typing import Any, Callable\n",
    "from jaxopt import OptaxSolver\n",
    "import functools\n",
    "import optax\n",
    "import relaxed\n",
    "from neos import data\n",
    "\n",
    "from inspect import getclosurevars\n",
    "\n",
    "pyhf.set_backend(\"jax\")\n",
    "\n",
    "def run_neos(\n",
    "    bandwidth,\n",
    "    bins,\n",
    "    epochs,\n",
    "    loss=\"log(cls)\",\n",
    "    rng=PRNGKey(0),\n",
    "    nn=None,\n",
    "    batch_size=500,\n",
    "    reflect=False,\n",
    "    num_points=100000,\n",
    "    animate=False,\n",
    "    plot=True,\n",
    "    test_size=0.1,\n",
    "    predict=None,\n",
    "    LUMI=10,\n",
    "    sig_mean=jnp.asarray([-1, 1]),\n",
    "    bup_mean=jnp.asarray([2.5, 2]),\n",
    "    bdown_mean=jnp.asarray([-2.5, -1.5]),\n",
    "    b_mean=jnp.asarray([1, -1]),\n",
    "    sig_scale=2,\n",
    "    bkg_scale=10,\n",
    "):\n",
    "\n",
    "    ## helper fn for data gen ##\n",
    "    def gen_blobs():\n",
    "        sig = jax.random.multivariate_normal(\n",
    "            rng, sig_mean, jnp.asarray([[1, 0], [0, 1]]), shape=(num_points,)\n",
    "        )\n",
    "        bkg_up = jax.random.multivariate_normal(\n",
    "            rng, bup_mean, jnp.asarray([[1, 0], [0, 1]]), shape=(num_points,)\n",
    "        )\n",
    "        bkg_down = jax.random.multivariate_normal(\n",
    "            rng, bdown_mean, jnp.asarray([[1, 0], [0, 1]]), shape=(num_points,)\n",
    "        )\n",
    "        bkg_nom = jax.random.multivariate_normal(\n",
    "            rng, b_mean, jnp.asarray([[1, 0], [0, 1]]), shape=(num_points,)\n",
    "        )\n",
    "\n",
    "        return sig, bkg_nom, bkg_up, bkg_down\n",
    "\n",
    "    ## nn --> yields ##\n",
    "    def histogram_maker(nn, data):\n",
    "        assert data\n",
    "        s, b_nom, b_up, b_down = data\n",
    "\n",
    "        nn_s, nn_b_nom, nn_b_up, nn_b_down = (\n",
    "            predict(nn, s).ravel(),\n",
    "            predict(nn, b_nom).ravel(),\n",
    "            predict(nn, b_up).ravel(),\n",
    "            predict(nn, b_down).ravel(),\n",
    "        )\n",
    "\n",
    "        kde_counts = [\n",
    "            relaxed.hist_kde(nn_s, bins, bandwidth, reflect_infinities=reflect)\n",
    "            * sig_scale\n",
    "            / num_points\n",
    "            * LUMI,\n",
    "            relaxed.hist_kde(nn_b_nom, bins, bandwidth, reflect_infinities=reflect)\n",
    "            * bkg_scale\n",
    "            / num_points\n",
    "            * LUMI,\n",
    "            relaxed.hist_kde(nn_b_up, bins, bandwidth, reflect_infinities=reflect)\n",
    "            * bkg_scale\n",
    "            / num_points\n",
    "            * LUMI,\n",
    "            relaxed.hist_kde(\n",
    "                nn_b_down,\n",
    "                bins,\n",
    "                bandwidth,\n",
    "                reflect_infinities=reflect,\n",
    "            )\n",
    "            * bkg_scale\n",
    "            / num_points\n",
    "            * LUMI,\n",
    "        ]\n",
    "\n",
    "        return [k for k in kde_counts]\n",
    "\n",
    "    ## yields --> model ##\n",
    "    def model(nn, data):\n",
    "        yields = histogram_maker(nn, data)\n",
    "        m = correlated_background(*yields)\n",
    "        nompars = m.config.suggested_init()\n",
    "        bonlypars = jnp.asarray([x for x in nompars])\n",
    "        bonlypars = jax.ops.index_update(bonlypars, m.config.poi_index, 0.0)\n",
    "        return m, bonlypars\n",
    "\n",
    "\n",
    "    ## Data generation + train/test ##\n",
    "    d = gen_blobs()\n",
    "\n",
    "    split = train_test_split(*d, test_size=test_size, shuffle=False, random_state=1)\n",
    "    train, test = split[::2], split[1::2]\n",
    "\n",
    "    num_train = train[0].shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "    # batching mechanism\n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(0)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "                yield [points[batch_idx] for points in train]\n",
    "\n",
    "    stream = data_stream()\n",
    "\n",
    "    # regression net\n",
    "    init_random_params, predict = stax.serial(\n",
    "        stax.Dense(1024),\n",
    "        stax.Relu,\n",
    "        stax.Dense(1024),\n",
    "        stax.Relu,\n",
    "        stax.Dense(1),\n",
    "        stax.Sigmoid,\n",
    "    )\n",
    "\n",
    "    _, network = init_random_params(jax.random.PRNGKey(2), (-1, 2))\n",
    "\n",
    "    Array = Any\n",
    "\n",
    "    ## -NLL from model ##\n",
    "    def fit_objective(model_pars, model_kwargs, constrained_mu):\n",
    "        m, bonlypars = model(model_pars, **model_kwargs)\n",
    "        exp_bonly_data = m.expected_data(bonlypars, include_auxdata=True)\n",
    "\n",
    "        def constrained_fit_objective(nuis_par: Array) -> float:  # NLL\n",
    "            pars = jnp.concatenate([jnp.asarray([constrained_mu]), jnp.array(nuis_par)])\n",
    "            return -m.logpdf(pars, data = exp_bonly_data)[0]\n",
    "\n",
    "        return constrained_fit_objective\n",
    "\n",
    "    # try wrapping obj with closure_convert\n",
    "    def minimize(objective_fn, np, lr):\n",
    "        converted_fn, m_pars = jax.closure_convert(objective_fn, np) \n",
    "        # m_pars seems to be empty, took that line from docs example\n",
    "        solver = OptaxSolver(fun=converted_fn, opt=optax.adam(lr), implicit_diff=True)\n",
    "        return solver.run(np, *m_pars)[0][0]\n",
    "\n",
    "    # constrained fit with grad descent via Optax\n",
    "    def fit(model_pars, model_kwargs, constrained_mu=1., lr=3e-4):\n",
    "        m, _ = model(model_pars, **model_kwargs) \n",
    "        suggested_init = [m.config.suggested_init()[0]]\n",
    "        obj = fit_objective(model_pars, model_kwargs, constrained_mu=constrained_mu)\n",
    "\n",
    "        return minimize(obj, suggested_init, lr)\n",
    "\n",
    "    # use fit result as loss\n",
    "    def loss(nn, data):\n",
    "        return fit(nn, dict(data=data))\n",
    "    \n",
    "    print(loss(network, test)) # fwd pass\n",
    "    return jax.value_and_grad(loss)(network, test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "run_neos(bandwidth=0.1, bins=jnp.linspace(0,1,4), epochs=2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.0011910244856098077\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(DeviceArray(-0.00119102, dtype=float64),\n",
       " [(DeviceArray([[ 0.03059435, -0.00975196,  0.01705084, ...,  0.01742486,\n",
       "                  0.00082944, -0.03244367],\n",
       "                [ 0.01347071,  0.00778273, -0.01803691, ..., -0.00638081,\n",
       "                 -0.00243054, -0.00561185]], dtype=float64),\n",
       "   DeviceArray([-0.0004559 ,  0.00733674, -0.01414513, ..., -0.00874408,\n",
       "                 0.00250869, -0.00237924], dtype=float64)),\n",
       "  (),\n",
       "  (DeviceArray([[-8.54850084e-04,  4.33896450e-05, -4.56369895e-07, ...,\n",
       "                 -5.68307526e-05, -2.38097885e-03, -6.88376352e-04],\n",
       "                [-3.26957951e-04, -1.23679453e-08, -1.07857719e-07, ...,\n",
       "                 -6.89681392e-05, -1.13554108e-03, -3.72788048e-04],\n",
       "                [-5.14473857e-05,  1.04615312e-11, -1.15888590e-08, ...,\n",
       "                 -9.54691167e-06, -1.78892791e-04, -5.77282605e-05],\n",
       "                ...,\n",
       "                [-3.30678975e-04,  1.05126151e-05, -3.43993512e-11, ...,\n",
       "                 -1.35945844e-05, -1.00762982e-03, -3.00488491e-04],\n",
       "                [ 2.14282113e-05, -1.38382156e-07, -1.01769242e-07, ...,\n",
       "                  8.48457030e-05,  1.19481919e-04, -4.86280093e-05],\n",
       "                [-1.67443743e-04,  1.16283085e-04, -7.03326583e-08, ...,\n",
       "                  1.67359804e-05,  1.94047250e-03,  7.12395396e-04]],            dtype=float64),\n",
       "   DeviceArray([-5.11543327e-03,  5.30762426e-04, -2.65828569e-05, ...,\n",
       "                 1.27171190e-03,  1.18723188e-03, -1.48965505e-04],            dtype=float64)),\n",
       "  (),\n",
       "  (DeviceArray([[-3.79520016e-03],\n",
       "                [ 3.54607604e-06],\n",
       "                [-4.16904097e-06],\n",
       "                ...,\n",
       "                [ 2.01197051e-03],\n",
       "                [ 2.90235312e-02],\n",
       "                [-1.21513200e-03]], dtype=float64),\n",
       "   DeviceArray([0.00250409], dtype=float64)),\n",
       "  ()])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import jax\n",
    "import jax.scipy as jsp\n",
    "from jaxopt import OptaxSolver\n",
    "import optax\n",
    "\n",
    "def pipeline(param_for_grad, data):\n",
    "    def to_minimize(latent):\n",
    "        return -jsp.stats.norm.logpdf(data, loc=param_for_grad*latent, scale=1)\n",
    "\n",
    "    solver = OptaxSolver(fun=to_minimize, opt=optax.adam(3e-4), implicit_diff=True)\n",
    "\n",
    "    initial, _ = solver.init(init_params = 5.)\n",
    "\n",
    "    result, _ = solver.run(init_params = initial)\n",
    "\n",
    "    return result\n",
    "\n",
    "jax.value_and_grad(pipeline)(2., data=6.)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "CustomVJPException",
     "evalue": "Detected differentiation of a custom_vjp function with respect to a closed-over value. That isn't supported because the custom VJP rule only specifies how to differentiate the custom_vjp function with respect to explicit input parameters. Try passing the closed-over value into the custom_vjp function as an argument, and adapting the custom_vjp fwd and bwd rules.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCustomVJPException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nk/l3bpn5t13lg1qclxj08k_b800000gn/T/ipykernel_60299/4204402221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "\u001b[0;32m/var/folders/nk/l3bpn5t13lg1qclxj08k_b800000gn/T/ipykernel_60299/4204402221.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(param_for_grad, data)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "\u001b[0;32m~/new-neos/neos/venv/lib/python3.9/site-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mpost_process_custom_vjp_call\u001b[0;34m(self, out_tracers, params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpost_process_custom_vjp_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mCustomVJPException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCustomVJPException\u001b[0m: Detected differentiation of a custom_vjp function with respect to a closed-over value. That isn't supported because the custom VJP rule only specifies how to differentiate the custom_vjp function with respect to explicit input parameters. Try passing the closed-over value into the custom_vjp function as an argument, and adapting the custom_vjp fwd and bwd rules."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "9773bb7d942b65d7c55a8ef7c6816693918976dd9514d0a390214a44c20ed301"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
