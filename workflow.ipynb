{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "from jax.experimental import stax\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as npr\n",
    "import pyhf\n",
    "from pyhf.simplemodels import correlated_background\n",
    "from jaxopt import OptaxSolver \n",
    "import optax\n",
    "import relaxed\n",
    "\n",
    "pyhf.set_backend(\"jax\")\n",
    "\n",
    "def run_4blobs(\n",
    "    bandwidth,\n",
    "    bins,\n",
    "    epochs,\n",
    "    loss_expr=lambda metrics: jnp.log(metrics['CLs']),\n",
    "    rng=PRNGKey(0),\n",
    "    nn=None,\n",
    "    batch_size=500,\n",
    "    reflect=False,\n",
    "    num_points=100000,\n",
    "    animate=False,\n",
    "    plot=True,\n",
    "    test_size=0.1,\n",
    "    predict=None,\n",
    "    LUMI=10,\n",
    "    sig_mean=jnp.asarray([-1, 1]),\n",
    "    bup_mean=jnp.asarray([2.5, 2]),\n",
    "    bdown_mean=jnp.asarray([-2.5, -1.5]),\n",
    "    b_mean=jnp.asarray([1, -1]),\n",
    "    sig_scale=2,\n",
    "    bkg_scale=10,\n",
    "):\n",
    "\n",
    "    ## helper fn for data gen ##\n",
    "    def gen_blobs():\n",
    "        sig = jax.random.multivariate_normal(\n",
    "            rng, sig_mean, jnp.asarray([[1, 0], [0, 1]]), shape=(num_points,)\n",
    "        )\n",
    "        bkg_up = jax.random.multivariate_normal(\n",
    "            rng, bup_mean, jnp.asarray([[1, 0], [0, 1]]), shape=(num_points,)\n",
    "        )\n",
    "        bkg_down = jax.random.multivariate_normal(\n",
    "            rng, bdown_mean, jnp.asarray([[1, 0], [0, 1]]), shape=(num_points,)\n",
    "        )\n",
    "        bkg_nom = jax.random.multivariate_normal(\n",
    "            rng, b_mean, jnp.asarray([[1, 0], [0, 1]]), shape=(num_points,)\n",
    "        )\n",
    "\n",
    "        return sig, bkg_nom, bkg_up, bkg_down\n",
    "\n",
    "    ## nn --> yields ##\n",
    "    def histogram_maker(nn, data):\n",
    "        assert data\n",
    "        s, b_nom, b_up, b_down = data\n",
    "\n",
    "        nn_s, nn_b_nom, nn_b_up, nn_b_down = (\n",
    "            predict(nn, s).ravel(),\n",
    "            predict(nn, b_nom).ravel(),\n",
    "            predict(nn, b_up).ravel(),\n",
    "            predict(nn, b_down).ravel(),\n",
    "        )\n",
    "\n",
    "        kde_counts = [\n",
    "            relaxed.hist_kde(nn_s, bins, bandwidth, reflect_infinities=reflect)\n",
    "            * sig_scale\n",
    "            / num_points\n",
    "            * LUMI,\n",
    "            relaxed.hist_kde(nn_b_nom, bins, bandwidth, reflect_infinities=reflect)\n",
    "            * bkg_scale\n",
    "            / num_points\n",
    "            * LUMI,\n",
    "            relaxed.hist_kde(nn_b_up, bins, bandwidth, reflect_infinities=reflect)\n",
    "            * bkg_scale\n",
    "            / num_points\n",
    "            * LUMI,\n",
    "            relaxed.hist_kde(\n",
    "                nn_b_down,\n",
    "                bins,\n",
    "                bandwidth,\n",
    "                reflect_infinities=reflect,\n",
    "            )\n",
    "            * bkg_scale\n",
    "            / num_points\n",
    "            * LUMI,\n",
    "        ]\n",
    "\n",
    "        return [k for k in kde_counts]\n",
    "\n",
    "    ## yields --> model ##\n",
    "    def model_with_bonlypars(nn, data):\n",
    "        yields = histogram_maker(nn, data)\n",
    "        m = correlated_background(*yields)\n",
    "        nompars = m.config.suggested_init()\n",
    "        bonlypars = jnp.asarray([x for x in nompars])\n",
    "        bonlypars = bonlypars.at[m.config.poi_index].set(0.0)\n",
    "        return m, bonlypars\n",
    "\n",
    "\n",
    "    ## Data generation + train/test (thanks to jax docs for batching code!) ##\n",
    "    d = gen_blobs()\n",
    "\n",
    "    split = train_test_split(*d, test_size=test_size, shuffle=False, random_state=1)\n",
    "    train, test = split[::2], split[1::2]\n",
    "\n",
    "    num_train = train[0].shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "    # batching mechanism\n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(0)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "                yield [points[batch_idx] for points in train]\n",
    "\n",
    "    stream = data_stream()\n",
    "\n",
    "    # regression net\n",
    "    init_random_params, predict = stax.serial(\n",
    "        stax.Dense(1024),\n",
    "        stax.Relu,\n",
    "        stax.Dense(1024),\n",
    "        stax.Relu,\n",
    "        stax.Dense(1),\n",
    "        stax.Sigmoid,\n",
    "    )\n",
    "\n",
    "    _, network = init_random_params(jax.random.PRNGKey(2), (-1, 2))\n",
    "\n",
    "    ## -NLL from model ##\n",
    "    def fit_objective(model_pars, model_kwargs, constrained_mu=None):\n",
    "        m, bonlypars = model_with_bonlypars(model_pars, **model_kwargs)\n",
    "        exp_bonly_data = m.expected_data(bonlypars, include_auxdata=True)\n",
    "\n",
    "        def constrained_fit_objective(nuis_par) -> float:  # NLL\n",
    "            pars = jnp.concatenate([jnp.asarray([constrained_mu]), jnp.array(nuis_par)])\n",
    "            return -m.logpdf(pars, data = exp_bonly_data)[0]\n",
    "\n",
    "        return constrained_fit_objective\n",
    "\n",
    "    # try wrapping obj with closure_convert\n",
    "    def _minimize(objective_fn, lhood_pars, lr):\n",
    "        converted_fn, m_pars = jax.closure_convert(objective_fn, lhood_pars) \n",
    "        # m_pars seems to be empty, took that line from docs example\n",
    "        solver = OptaxSolver(fun=converted_fn, opt=optax.adam(lr), implicit_diff=True)\n",
    "        return solver.run(lhood_pars, *m_pars)[0]\n",
    "\n",
    "    # constrained fit with grad descent via Optax\n",
    "    def fit(model_pars, model_kwargs, init_vals, constrained_mu=None, lr=3e-4):\n",
    "        obj = fit_objective(model_pars, model_kwargs, constrained_mu=constrained_mu)\n",
    "        fit_res = _minimize(obj, init_vals, lr)\n",
    "        new_pars = jnp.array([constrained_mu, fit_res[0]]) if constrained_mu else fit_res \n",
    "        return new_pars\n",
    "\n",
    "    # # use fit result as loss\n",
    "    # def loss(nn, data):\n",
    "    #     return fit(nn, dict(data=data))\n",
    "    \n",
    "    # print(loss(network, test)) # fwd pass\n",
    "\n",
    "    \n",
    "    def expected_CLs(nn, model_kwargs=dict(), test_mu=1., fit_lr=3e-4, loss_expr=lambda metrics: metrics['CLs']):\n",
    "        m, bonlypars = model_with_bonlypars(nn, **model_kwargs) \n",
    "        exp_bonly_data = m.expected_data(bonlypars, include_auxdata=True)\n",
    "        suggested_init = m.config.suggested_init()\n",
    "        del suggested_init[0] # don't need init for mu since we're not fitting it\n",
    "        \n",
    "        # we know that the global MLE pars for expected bkg-only data are the bkg-only pars!\n",
    "        denominator = bonlypars\n",
    "\n",
    "        numerator = fit(nn, model_kwargs, init_vals=suggested_init, constrained_mu=test_mu, lr=fit_lr)\n",
    "\n",
    "        # compute test statistic (lambda(µ))\n",
    "        profile_likelihood = -2 * (\n",
    "            m.logpdf(numerator, exp_bonly_data)[0] - m.logpdf(denominator, exp_bonly_data)[0]\n",
    "        )\n",
    "\n",
    "        # in exclusion fit zero out test stat if best fit µ^ is larger than test µ\n",
    "        muhat = denominator[0]\n",
    "        sqrtqmu = jnp.sqrt(jnp.where(muhat < test_mu, profile_likelihood, 0.0))\n",
    "        CLsb = 1 - pyhf.tensorlib.normal_cdf(sqrtqmu)\n",
    "        altval = 0\n",
    "        CLb = 1 - pyhf.tensorlib.normal_cdf(altval)\n",
    "        CLs = CLsb / CLb\n",
    "        pull = jnp.array(\n",
    "            [\n",
    "                (numerator - jnp.array(m.config.suggested_init()))[\n",
    "                    m.config.par_order.index(k)\n",
    "                ]\n",
    "                / m.config.param_set(k).width()[0]\n",
    "                for k in m.config.par_order\n",
    "                if m.config.param_set(k).constrained\n",
    "            ]\n",
    "        )\n",
    "        # should use global mle pars -- here we know them since exp_data came from bonlypars\n",
    "        errors = relaxed.cramer_rao_uncert(m, bonlypars, exp_bonly_data)\n",
    "\n",
    "        pull_err = jnp.array(\n",
    "            [\n",
    "                errors[m.config.par_slice(k)] / m.config.param_set(k).width()[0]\n",
    "                for k in m.config.par_order\n",
    "                if m.config.param_set(k).constrained\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        metrics = dict(\n",
    "            CLs=CLs,\n",
    "            CLsb=CLsb,\n",
    "            CLb=CLb,\n",
    "            profile_likelihood=profile_likelihood,\n",
    "            pull=pull,\n",
    "            pull_err=pull_err,\n",
    "            errors=errors,\n",
    "        )\n",
    "\n",
    "        return loss_expr(metrics), metrics\n",
    "\n",
    "    def loss(nn, data, loss_expr):\n",
    "        return expected_CLs(nn, dict(data=data), test_mu=1., loss_expr=loss_expr)\n",
    "\n",
    "    return jax.value_and_grad(loss, has_aux = True)(network, test, loss_expr)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "run_4blobs(bandwidth=0.1, bins=jnp.linspace(0,1,4), epochs=2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((DeviceArray(-1.36451481, dtype=float64),\n",
       "  {'CLb': DeviceArray(0.5, dtype=float64),\n",
       "   'CLs': DeviceArray(0.25550461, dtype=float64),\n",
       "   'CLsb': DeviceArray(0.12775231, dtype=float64),\n",
       "   'errors': DeviceArray([0.96647312, 1.57786485], dtype=float64),\n",
       "   'profile_likelihood': DeviceArray(1.29295218, dtype=float64),\n",
       "   'pull': DeviceArray([1.], dtype=float64),\n",
       "   'pull_err': DeviceArray([[0.96647312]], dtype=float64)}),\n",
       " [(DeviceArray([[-0.07969591,  0.0009669 , -0.00181384, ..., -0.01683938,\n",
       "                  0.02609834,  0.08186976],\n",
       "                [-0.09461673,  0.00064717, -0.00298575, ..., -0.02308915,\n",
       "                 -0.02503954,  0.06435425]], dtype=float64),\n",
       "   DeviceArray([-0.0341202 , -0.00259844,  0.00235879, ..., -0.00706263,\n",
       "                 0.02045382,  0.02705943], dtype=float64)),\n",
       "  (),\n",
       "  (DeviceArray([[-1.17804279e-04, -1.65407495e-04,  8.04878312e-07, ...,\n",
       "                  3.25869044e-05, -2.83761319e-03, -7.70001130e-04],\n",
       "                [ 1.09952157e-04,  5.70949319e-08,  1.72340420e-07, ...,\n",
       "                  1.66949532e-04,  3.66926026e-04,  4.64565376e-05],\n",
       "                [ 1.09933681e-05,  1.17533907e-10,  1.65138875e-08, ...,\n",
       "                  1.55877725e-05,  3.63878988e-05,  6.36384894e-06],\n",
       "                ...,\n",
       "                [-5.09493719e-05, -4.01119867e-05,  6.48803884e-13, ...,\n",
       "                  3.61233153e-06, -7.78434368e-04, -1.89659398e-04],\n",
       "                [ 8.79291276e-04,  4.94849894e-07,  2.22817045e-07, ...,\n",
       "                  1.76953930e-03,  3.46812499e-03,  2.89468010e-05],\n",
       "                [-1.63906965e-04, -4.43819347e-04,  9.17289743e-08, ...,\n",
       "                  3.30513675e-05, -1.02631412e-02, -3.32588687e-03]],            dtype=float64),\n",
       "   DeviceArray([ 1.24338027e-02, -2.00282982e-03,  5.24054128e-05, ...,\n",
       "                 3.28919470e-02, -2.19371114e-03, -2.05117551e-02],            dtype=float64)),\n",
       "  (),\n",
       "  (DeviceArray([[ 3.34984835e-02],\n",
       "                [-1.34236044e-05],\n",
       "                [ 9.32867931e-06],\n",
       "                ...,\n",
       "                [ 3.54330167e-02],\n",
       "                [-5.12937332e-02],\n",
       "                [-3.11744550e-02]], dtype=float64),\n",
       "   DeviceArray([0.00139393], dtype=float64)),\n",
       "  ()])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "9773bb7d942b65d7c55a8ef7c6816693918976dd9514d0a390214a44c20ed301"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
