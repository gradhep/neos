{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neos.fit\n",
    "\n",
    "> Module containing functions to perform maximum likelihod fits in a differentiable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fax` module is leveraged to calculate gradients of maximum likelihood fits through the 'two-phase' method, which is a technique for automatic differentiation of functions with an attractive fixed point, i.e. they satisfy $f(x) - x = 0$. This is the case for a minimization routine, where the minimization occurs from the minimum: `minimize(f, x_init) = x_min` --> `minimize(f, x_init=x_min) = x_min`. You can imagine that the initial iterations won't give any useful information in terms of the gradient of the minimum; this approach leverages this fact by performing the minimization a second time in the neighborhood of the fixed point, and keeping track of gradients there, which avoids the unnecessary unrolling of many loops for the early iteration.\n",
    "\n",
    "To read more about this method, see section 2.3 of [this paper](https://www.researchgate.net/profile/Ala_Taftaf2/publication/323176030_ADJOINTS_OF_FIXED-POINT_ITERATIONS/links/5a8465644585159152b7fe00/ADJOINTS-OF-FIXED-POINT-ITERATIONS.pdf) that looks at some methods to approach the fixed-point differation problem within automatic differentiation.\n",
    "\n",
    "The fits themselves are done by gradient descent with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from fax.implicit import twophase\n",
    "import jax.experimental.optimizers as optimizers\n",
    "\n",
    "from neos.transforms import to_bounded_vec, to_inf_vec, to_bounded, to_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def global_fit(\n",
    "    model_constructor,\n",
    "    pdf_transform=False,\n",
    "    default_rtol=1e-10,\n",
    "    default_atol=1e-10,\n",
    "    default_max_iter=int(1e7),\n",
    "    learning_rate=1e-6,\n",
    "):\n",
    "    \"\"\"Wraps a series of functions that perform maximum likelihood fitting in\n",
    "    the `two_phase_solver` method found in the `fax` python module. This allows\n",
    "    for the calculation of gradients of the best-fit parameters with respect to\n",
    "    upstream parameters that control the underlying model, i.e. the event\n",
    "    yields (which are then parameterized by weights or similar).\n",
    "\n",
    "    Args:\n",
    "        model_constructor: Function that takes in the parameters of the observable,\n",
    "        and returns a model object (and background-only parameters)\n",
    "    Returns:\n",
    "        global_fitter: Callable function that performs global fits.\n",
    "        Differentiable :)\n",
    "    \"\"\"\n",
    "\n",
    "    adam_init, adam_update, adam_get_params = optimizers.adam(learning_rate)\n",
    "\n",
    "    def make_model(model_pars):\n",
    "        m, bonlypars = model_constructor(model_pars)\n",
    "\n",
    "        bounds = m.config.suggested_bounds()\n",
    "\n",
    "        exp_bonly_data = m.expected_data(bonlypars, include_auxdata=True)\n",
    "\n",
    "        def expected_logpdf(pars):  # maps pars to bounded space if pdf_transform = True\n",
    "\n",
    "            return (\n",
    "                m.logpdf(to_bounded_vec(pars, bounds), exp_bonly_data)\n",
    "                if pdf_transform\n",
    "                else m.logpdf(pars, exp_bonly_data)\n",
    "            )\n",
    "\n",
    "        def global_fit_objective(pars):  # NLL\n",
    "            return -expected_logpdf(pars)[0]\n",
    "\n",
    "        return global_fit_objective\n",
    "\n",
    "    def global_bestfit_minimized(hyper_param):\n",
    "        nll = make_model(hyper_param)\n",
    "\n",
    "        def bestfit_via_grad_descent(i, param):  # gradient descent\n",
    "            g = jax.grad(nll)(param)\n",
    "            # param = param - g * learning_rate\n",
    "            param = adam_get_params(adam_update(i, g, adam_init(param)))\n",
    "            return param\n",
    "\n",
    "        return bestfit_via_grad_descent\n",
    "\n",
    "    global_solve = twophase.two_phase_solver(\n",
    "        param_func=global_bestfit_minimized,\n",
    "        default_rtol=default_rtol,\n",
    "        default_atol=default_atol,\n",
    "        default_max_iter=default_max_iter,\n",
    "    )\n",
    "\n",
    "    def global_fitter(init, hyper_pars):\n",
    "        solve = global_solve(init, hyper_pars)\n",
    "        return solve.value\n",
    "\n",
    "    return global_fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def constrained_fit(\n",
    "    model_constructor,\n",
    "    pdf_transform=False,\n",
    "    default_rtol=1e-10,\n",
    "    default_atol=1e-10,\n",
    "    default_max_iter=int(1e7),\n",
    "    learning_rate=1e-6,\n",
    "):\n",
    "    \"\"\"Wraps a series of functions that perform maximum likelihood fitting in\n",
    "    the `two_phase_solver` method found in the `fax` python module. This allows\n",
    "    for the calculation of gradients of the best-fit parameters with respect to\n",
    "    upstream parameters that control the underlying model, i.e. the event\n",
    "    yields (which are then parameterized by weights or similar).\n",
    "\n",
    "    Args:\n",
    "        model_constructor: Function that takes in the parameters of the observable,\n",
    "        and returns a model object (and background-only parameters)\n",
    "    Returns:\n",
    "        constrained_fitter: Callable function that performs constrained fits.\n",
    "        Differentiable :)\n",
    "    \"\"\"\n",
    "\n",
    "    adam_init, adam_update, adam_get_params = optimizers.adam(learning_rate)\n",
    "\n",
    "    def make_model(hyper_pars):\n",
    "\n",
    "        model_pars, constrained_mu = hyper_pars\n",
    "        m, bonlypars = model_constructor(model_pars)\n",
    "\n",
    "        bounds = m.config.suggested_bounds()\n",
    "        constrained_mu = (\n",
    "            to_inf(constrained_mu, bounds[0]) if pdf_transform else constrained_mu\n",
    "        )\n",
    "\n",
    "        exp_bonly_data = m.expected_data(bonlypars, include_auxdata=True)\n",
    "\n",
    "        def expected_logpdf(pars):  # maps pars to bounded space if pdf_transform = True\n",
    "\n",
    "            return (\n",
    "                m.logpdf(to_bounded_vec(pars, bounds), exp_bonly_data)\n",
    "                if pdf_transform\n",
    "                else m.logpdf(pars, exp_bonly_data)\n",
    "            )\n",
    "\n",
    "        def constrained_fit_objective(nuis_par):  # NLL\n",
    "            pars = jnp.concatenate([jnp.asarray([constrained_mu]), nuis_par])\n",
    "            return -expected_logpdf(pars)[0]\n",
    "\n",
    "        return constrained_mu, constrained_fit_objective\n",
    "\n",
    "    def constrained_bestfit_minimized(hyper_pars):\n",
    "        mu, cnll = make_model(hyper_pars)\n",
    "\n",
    "        def bestfit_via_grad_descent(i, param):  # gradient descent\n",
    "            _, np = param[0], param[1:]\n",
    "            g = jax.grad(cnll)(np)\n",
    "            np = adam_get_params(adam_update(i, g, adam_init(np)))\n",
    "            param = jnp.concatenate([jnp.asarray([mu]), np])\n",
    "            return param\n",
    "\n",
    "        return bestfit_via_grad_descent\n",
    "\n",
    "    constrained_solver = twophase.two_phase_solver(\n",
    "        param_func=constrained_bestfit_minimized,\n",
    "        default_rtol=default_rtol,\n",
    "        default_atol=default_atol,\n",
    "        default_max_iter=default_max_iter,\n",
    "    )\n",
    "\n",
    "    def constrained_fitter(init, hyper_pars):\n",
    "        solve = constrained_solver(init, hyper_pars)\n",
    "        return solve.value\n",
    "\n",
    "    return constrained_fitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained param fit (mu=1): [1.         0.02099289]\n",
      "Global param fit: [-1.44533048e-06  1.29791259e-06]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "from jax.experimental import stax\n",
    "\n",
    "import neos\n",
    "from neos.makers import hists_from_nn, histosys_model_from_hists\n",
    "from neos.data import generate_blobs\n",
    "from neos.fit import global_fit, constrained_fit\n",
    "\n",
    "# data generator\n",
    "gen_data = generate_blobs(rng=PRNGKey(1),blobs=4)\n",
    "\n",
    "# nn\n",
    "init_random_params, predict = stax.serial(\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1),\n",
    "    stax.Sigmoid\n",
    ")\n",
    "\n",
    "# instantiate model maker\n",
    "hist_maker = hists_from_nn(gen_data, predict, method='kde')\n",
    "model_maker = histosys_model_from_hists(hist_maker)\n",
    "\n",
    "# grab parameters for nn and kde\n",
    "_, network = init_random_params(jax.random.PRNGKey(13), (-1, 2))\n",
    "hyperpars = dict(bandwidth=0.5, bins=jnp.linspace(0,1,3))\n",
    "\n",
    "# fitting time!\n",
    "g_fitter = global_fit(model_maker)\n",
    "c_fitter = constrained_fit(model_maker)\n",
    "\n",
    "# make model\n",
    "m, bonlypars = model_maker([network, hyperpars])\n",
    "\n",
    "initval = jnp.asarray([1.0, 1.0])\n",
    "\n",
    "# the constrained fit\n",
    "print(f'Constrained param fit (mu=1): {c_fitter(initval, [[network, hyperpars], 1])}')\n",
    "\n",
    "# global fit\n",
    "print(f'Global param fit: {g_fitter(initval, [network, hyperpars])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
