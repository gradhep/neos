---

title: neos


keywords: fastai
sidebar: home_sidebar

summary: "~neural~ nice end-to-end optimized statistics"
description: "~neural~ nice end-to-end optimized statistics"
nb_path: "nbs/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://zenodo.org/badge/latestdoi/235776682"><img src="https://zenodo.org/badge/235776682.svg" alt="DOI"></a> <img src="https://github.com/pyhf/neos/workflows/CI/badge.svg" alt="CI"> <a href="https://mybinder.org/v2/gh/pyhf/neos/master?filepath=demo_training.ipynb"><img src="https://mybinder.org/badge_logo.svg" alt="Binder"></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html alt="neos logo" max-width="250" file="/neos/assets/neos_logo.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/neos/assets/pyhf_3.gif" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="About">About<a class="anchor-link" href="#About"> </a></h2><p>Leverages the shoulders of giants (<code>jax</code>, <code>fax</code>, and <code>pyhf</code>) to differentiate through a high-energy physics analysis workflow, including the construction of the frequentist profile likelihood.</p>
<p>Documentation can be found within the notebooks in the nbs folder, or (temporarily) at <a href="phinate.github.io/neos">phinate.github.io/neos</a>.</p>
<p>To see examples of neos in action, look for the notebooks in the nbs folder with the <code>demo_</code> prefix.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2><p>Just run</p>

<pre><code>python -m pip install neos</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Contributing">Contributing<a class="anchor-link" href="#Contributing"> </a></h2><p><strong>Please read</strong> <code>CONTRIBUTING.md</code> <strong>before making a PR</strong>, as this project is maintained using <code>nbdev</code>, which operates completely using Jupyter notebooks. One should make their changes in the corresponding notebooks in the <code>nbs</code> folder (including <code>README</code> changes -- see <code>nbs/index.ipynb</code>), and not in the library code, which is automatically generated.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Example-usage----train-a-neural-network-to-optimize-an-expected-p-value">Example usage -- train a neural network to optimize an expected p-value<a class="anchor-link" href="#Example-usage----train-a-neural-network-to-optimize-an-expected-p-value"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.experimental.optimizers</span> <span class="k">as</span> <span class="nn">optimizers</span>
<span class="kn">import</span> <span class="nn">jax.experimental.stax</span> <span class="k">as</span> <span class="nn">stax</span>
<span class="kn">import</span> <span class="nn">jax.random</span>
<span class="kn">from</span> <span class="nn">jax.random</span> <span class="kn">import</span> <span class="n">PRNGKey</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">pyhf</span>
<span class="n">pyhf</span><span class="o">.</span><span class="n">set_backend</span><span class="p">(</span><span class="s1">&#39;jax&#39;</span><span class="p">)</span>
<span class="n">pyhf</span><span class="o">.</span><span class="n">default_backend</span> <span class="o">=</span> <span class="n">pyhf</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">jax_backend</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="s1">&#39;64b&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">neos</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">infer</span><span class="p">,</span> <span class="n">makers</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">PRNGKey</span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's start by making a basic neural network for regression with the <code>stax</code> module found in <code>jax</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">init_random_params</span><span class="p">,</span> <span class="n">predict</span> <span class="o">=</span> <span class="n">stax</span><span class="o">.</span><span class="n">serial</span><span class="p">(</span>
    <span class="n">stax</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1024</span><span class="p">),</span>
    <span class="n">stax</span><span class="o">.</span><span class="n">Relu</span><span class="p">,</span>
    <span class="n">stax</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1024</span><span class="p">),</span>
    <span class="n">stax</span><span class="o">.</span><span class="n">Relu</span><span class="p">,</span>
    <span class="n">stax</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">stax</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, let's compose a workflow that can make use of this network in a typical high-energy physics statistical analysis.</p>
<p>Our workflow is as follows:</p>
<ul>
<li>From a set of normal distributions with different means, we'll generate four blobs of <code>(x,y)</code> points, corresponding to a signal process, a nominal background process, and two variations of the background from varying the background distribution's mean up and down.</li>
<li>We'll then feed these points into the previously defined neural network for each blob, and construct a histogram of the output using kernel density estimation. The difference between the two background variations is used as a systematic uncertainty on the nominal background.</li>
<li>We can then leverage the magic of <code>pyhf</code> to construct an <a href="https://scikit-hep.org/pyhf/intro.html#histfactory">event-counting statistical model</a> from the histogram yields.</li>
<li>Finally, we calculate the p-value of a test between the nominal signal and background-only hypotheses. This uses a <a href="https://arxiv.org/abs/1007.1727">profile likelihood-based test statistic</a>. </li>
</ul>
<p>In code, <code>neos</code> can specify this workflow through function composition:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data_gen</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">generate_blobs</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span><span class="n">blobs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># histogram maker</span>
<span class="n">hist_maker</span> <span class="o">=</span> <span class="n">makers</span><span class="o">.</span><span class="n">hists_from_nn</span><span class="p">(</span><span class="n">data_gen</span><span class="p">,</span> <span class="n">predict</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;kde&#39;</span><span class="p">)</span>
<span class="c1"># statistical model maker</span>
<span class="n">model_maker</span> <span class="o">=</span> <span class="n">makers</span><span class="o">.</span><span class="n">histosys_model_from_hists</span><span class="p">(</span><span class="n">hist_maker</span><span class="p">)</span>
<span class="c1"># CLs value getter</span>
<span class="n">get_cls</span> <span class="o">=</span> <span class="n">infer</span><span class="o">.</span><span class="n">expected_CLs</span><span class="p">(</span><span class="n">model_maker</span><span class="p">,</span> <span class="n">solver_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">pdf_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A peculiarity to note is that each of the functions used in this step actually return functions themselves. The reason we do this is that we need a skeleton of the workflow with all of the fixed parameters to be in place before calculating the loss function, as the only 'moving parts' here are the weights of the neural network.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>neos</code> also lets you specify hyperparameters for the histograms (e.g. binning, bandwidth) to allow these to be tuned throughout the learning process if neccesary (we don't do that here).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># three bins in the range [0,1]</span>
<span class="n">bandwidth</span> <span class="o">=</span> <span class="mf">0.27</span> <span class="c1"># smoothing parameter</span>
<span class="n">get_loss</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">get_cls</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span><span class="n">bandwidth</span><span class="o">=</span><span class="n">bandwidth</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our loss currently returns a list of metrics -- let's just index into the first one (the CLs value).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">test_mu</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">get_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">test_mu</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we just need to initialize the network's weights, and construct a training loop &amp; optimizer:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">network</span> <span class="o">=</span> <span class="n">init_random_params</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># init optimizer</span>
<span class="n">opt_init</span><span class="p">,</span> <span class="n">opt_update</span><span class="p">,</span> <span class="n">opt_params</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># define train loop</span>
<span class="k">def</span> <span class="nf">train_network</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">cls_vals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">network</span> <span class="o">=</span> <span class="n">init_random_params</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># parameter update function</span>
    <span class="k">def</span> <span class="nf">update_and_value</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">opt_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)(</span><span class="n">net</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">opt_update</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">state</span><span class="p">),</span> <span class="n">value</span><span class="p">,</span> <span class="n">net</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">network</span> <span class="o">=</span> <span class="n">update_and_value</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">epoch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">losses</span><span class="p">}</span>
        <span class="k">yield</span> <span class="n">network</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">epoch_time</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's time to train!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">maxN</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># make me bigger for better results (*nearly* true ;])</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">epoch_time</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_network</span><span class="p">(</span><span class="n">maxN</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;epoch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;CLs = </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">, took </span><span class="si">{</span><span class="n">epoch_time</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch 0: CLs = 0.06885, took 13.4896s
epoch 1: CLs = 0.03580, took 1.9772s
epoch 2: CLs = 0.01728, took 1.9912s
epoch 3: CLs = 0.00934, took 1.9947s
epoch 4: CLs = 0.00561, took 1.9548s
epoch 5: CLs = 0.00378, took 1.9761s
epoch 6: CLs = 0.00280, took 1.9500s
epoch 7: CLs = 0.00224, took 1.9844s
epoch 8: CLs = 0.00190, took 1.9913s
epoch 9: CLs = 0.00168, took 1.9928s
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And there we go!</p>
<p>You'll notice the first epoch seems to have a much larger training time. This is because <code>jax</code> is being used to just-in-time compile some of the code, which is an overhead that only needs to happen once.</p>
<p>If you want to reproduce the full animation from the top of this README, a version of this code with plotting helpers can be found in <code>demo_kde_pyhf.ipynb</code>! :D</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Thanks">Thanks<a class="anchor-link" href="#Thanks"> </a></h2><p>A big thanks to the teams behind <code>jax</code>, <code>fax</code>, and <code>pyhf</code> for their software and support.</p>

</div>
</div>
</div>
</div>
 

