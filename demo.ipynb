{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey, multivariate_normal\n",
    "import pyhf\n",
    "from typing import Callable, Any, Generator, Iterable\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import neos\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "pyhf.set_backend(\"jax\")\n",
    "\n",
    "# matplotlib settings\n",
    "plt.rc(\"figure\", figsize=(6, 3), dpi=150, facecolor=\"w\")\n",
    "plt.rc(\"legend\", fontsize=6)\n",
    "\n",
    "Array = jnp.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`neos` tackles the problem of learning optimal summary statistics from data. The workflow is explained in more detail [in our paper]() -- this notebook is designed to get you up-and-running with how that workflow looks in code, including how to do this for your own use-case! (if you want to do that for real, please reach out -- we'd love to help you!)\n",
    "\n",
    "To construct this kind of workflow, you need to specify three things:\n",
    "- How to construct your summary statistic from data\n",
    "- How to build your likelihood function from that summary statistic\n",
    "- A choice of metric relating to how you deem your analysis to be \"optimal\"\n",
    "\n",
    "The second bullet in-particular is very important; while we've abstracted away all the technical detail, `neos` is still not a drop-in loss function in the typical sense, since it requires *detailed information from the analyser on how to build the likelihood*. No longer do we have a problem-agnostic notion of \"signal\" and \"background\"; instead, we're wanting to optimise our specific problem, so we need to provide specific information. That is how `neos` is \"systematic-aware\" -- it's a technique that explicitly takes into account how your systematics are modelled. But to do this, we of course need to model them in the first place!\n",
    "\n",
    "Here, we're going to re-implement the example from the paper with 2-dimensional Gaussian blobs making up our data. We have a nominal estimate for signal and background data, and we also provide \"up\" and \"down\" variations that correspond to moving the mean of the background blob. The helper function to generate this dataset is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(\n",
    "    rng: int = 0,\n",
    "    num_points: int = 10000,\n",
    "    sig_mean: tuple[float, float] = (-1, 1),\n",
    "    bup_mean: tuple[float, float] = (2.5, 2),\n",
    "    bdown_mean: tuple[float, float] = (-2.5, -1.5),\n",
    "    b_mean: tuple[float, float] = (1, -1),\n",
    ") -> tuple[Array, Array, Array, Array]:\n",
    "    sig = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(sig_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "    bkg_up = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(bup_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "    bkg_down = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(bdown_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "\n",
    "    bkg_nom = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(b_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "    return sig, bkg_nom, bkg_up, bkg_down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we need to specify how we construct our summary statistic. We will use a neural network for this, left unspecified until later. Since our likelihood modelling stage will later involve using histograms, we'll also construct a histogram of the output, then scale the yields in each batch for a more \"realistic\" analysis. This step is wrapped up in a simple convenience function called `neos.hists_from_nn`, which we'll use in a moment.\n",
    "\n",
    "After this comes the problem-specific part: the likelihood model construction. I mentioned before that we have \"up\" and \"down\" datasets -- this is a typical construct in HEP that we use to model the effect of physical parameters. We will then have three histograms for the background data: one for each of the nominal, up, and down samples. The HistFactory prescription for building likelihood functions then models the uncertainty between these by interpolating between their yields. Luckily, this is done under the hood for us in `pyhf` -- we just need to write a dictionary specifying the format of our histograms. This can be found below for our case, where we describe our uncertainty by forming a nuisance parameter `\"correlated_bkg_uncertainty\"` that modifies the shape of the histogram in a bin-correlated fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we give a dict of histograms with keys \"sig\", \"bkg_nominal\", \"bkg_up\", \"bkg_down\".\n",
    "def model_from_hists(hists: dict[str, Array]) -> pyhf.Model:\n",
    "    \"\"\"How to make your HistFactory model from your histograms.\"\"\"\n",
    "    spec = {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",  # we only have one \"channel\" (data region)\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"signal\",\n",
    "                        \"data\": hists[\"sig\"],  # signal\n",
    "                        \"modifiers\": [\n",
    "                            {\n",
    "                                \"name\": \"mu\",\n",
    "                                \"type\": \"normfactor\",\n",
    "                                \"data\": None,\n",
    "                            },  # our signal strength modifier (parameter of interest)\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        \"data\": hists[\"bkg_nominal\"],  # background\n",
    "                        \"modifiers\": [\n",
    "                            {\n",
    "                                \"name\": \"correlated_bkg_uncertainty\",\n",
    "                                \"type\": \"histosys\",\n",
    "                                \"data\": {\n",
    "                                    \"hi_data\": hists[\"bkg_up\"],  # up sample\n",
    "                                    \"lo_data\": hists[\"bkg_down\"],  # down sample\n",
    "                                },\n",
    "                            },\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    return pyhf.Model(spec, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to form our metric of choice from our likelihood model. Here, we have a variety to choose from: uncertainty on the signal strength modifier (cross-section), discovery significance, CLs, even the classic binary cross-entropy (that doesn't use the likelihood at all). Instead of overloading the notebook with losses, I've baked them all into `neos.loss_from_model`, which takes in a `pyhf` model and a string with the metric you want to calculate.\n",
    "\n",
    "With these ingredients, we just need to compose them in one function, so we can take the gradient of that function with respect to our free parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    pars: dict[str, Array],\n",
    "    data: tuple[Array, ...],\n",
    "    nn: Callable,\n",
    "    loss: str | Callable,\n",
    "    bandwidth: float,\n",
    "    sample_names: Iterable,\n",
    "    scale_factors: dict[str, float],\n",
    "    bins: Array | None = None,  # in case you don't want to optimise binning\n",
    "    lumi: float = 10.0,  # overall scale factor\n",
    ") -> float:\n",
    "    data_dct = {k: v for k, v in zip(sample_names, data)}\n",
    "    if loss.lower() in [\"bce\", \"binary cross-entropy\"]:\n",
    "        return neos.losses.bce(data=data_dct, pars=pars[\"nn_pars\"], nn=nn)\n",
    "    hists = neos.hists_from_nn(\n",
    "        pars=pars[\"nn_pars\"],\n",
    "        nn=nn,\n",
    "        data=data_dct,\n",
    "        bandwidth=bandwidth,\n",
    "        bins=jnp.array([0, *pars[\"bins\"], 1]) if \"bins\" in pars else bins,\n",
    "        scale_factors=scale_factors,\n",
    "        overall_scale=lumi,\n",
    "    )\n",
    "    model = model_from_hists(hists)\n",
    "    return neos.loss_from_model(model, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is now mostly training boilerplate! I'll add titles, but should be mostly clear what's going on (ask if not!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network architecture + params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax\n",
    "\n",
    "rng_state = 0  # random state\n",
    "\n",
    "# feel free to modify :)\n",
    "init_random_params, nn = stax.serial(\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1),\n",
    "    stax.Sigmoid,\n",
    ")\n",
    "\n",
    "num_features = 2\n",
    "_, init = init_random_params(PRNGKey(rng_state), (-1, num_features))\n",
    "init_pars = dict(nn_pars=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define batching mechanism for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as npr\n",
    "\n",
    "batch_size = 2000\n",
    "data = generate_data(rng=rng_state, num_points=10000)\n",
    "split = train_test_split(*data, random_state=rng_state)\n",
    "train, test = split[::2], split[1::2]\n",
    "\n",
    "\n",
    "def batches(training_data: Array, batch_size: int) -> Generator:\n",
    "    num_train = training_data[0].shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "    # batching mechanism\n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(rng_state)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "                yield [points[batch_idx] for points in train]\n",
    "\n",
    "    return data_stream()\n",
    "\n",
    "\n",
    "batch_iterator = batches(train, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop -- change hyperparameters at will!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxopt import OptaxSolver\n",
    "import optax\n",
    "from time import perf_counter\n",
    "\n",
    "include_bins = True\n",
    "bins = jnp.linspace(0, 1, 5)  # keep in [0,1] if using sigmoid activation\n",
    "lr = 1e-3\n",
    "num_steps = 20\n",
    "# can choose from \"CLs\", \"discovery\", \"poi_uncert\" [approx. uncert. on mu], \"bce\" [classifier]\n",
    "objective = \"cls\"\n",
    "\n",
    "# the same keys you used in the model building step [model_from_hists]\n",
    "data_types = [\"sig\", \"bkg_nominal\", \"bkg_up\", \"bkg_down\"]\n",
    "scales = {k: 2.0 if k == \"sig\" else 10.0 for k in data_types}\n",
    "loss = partial(\n",
    "    pipeline,\n",
    "    nn=nn,\n",
    "    sample_names=data_types,\n",
    "    scale_factors=scales,\n",
    ")\n",
    "\n",
    "solver = OptaxSolver(loss, opt=optax.adam(lr), jit=True)\n",
    "\n",
    "pyhf.set_backend(\"jax\", default=True)\n",
    "\n",
    "if include_bins:\n",
    "    init_pars[\"bins\"] = bins[\n",
    "        1:-1\n",
    "    ]  # don't want to float endpoints [will account for kde spill]\n",
    "    state = solver.init_state(init_pars)\n",
    "else:\n",
    "    if \"bins\" in init_pars:\n",
    "        del init_pars[\"bins\"]\n",
    "    state = solver.init_state(init_pars)\n",
    "\n",
    "params = init_pars\n",
    "best_params = init_pars\n",
    "best_sig = 999\n",
    "metrics = {k: [] for k in [\"cls\", \"discovery\", \"poi_uncert\"]}\n",
    "\n",
    "for i in range(num_steps):\n",
    "    print(f\"step {i}: loss={objective}\")\n",
    "    data = next(batch_iterator)\n",
    "    start = perf_counter()\n",
    "    params, state = solver.update(\n",
    "        params, state, bins=bins, data=data, loss=objective, bandwidth=9e-2\n",
    "    )\n",
    "    end = perf_counter()\n",
    "    print(f\"update took {end-start:.4g}s\")\n",
    "    if \"bins\" in params:\n",
    "        print(\"bin edges: [0 \", *[f\"{f:.3g}\" for f in params[\"bins\"]], \" 1]\")\n",
    "    for metric in metrics:\n",
    "        test_metric = loss(params, bins=bins, data=test, loss=metric, bandwidth=1e-8)\n",
    "        print(f\"{metric}={test_metric:.4g}\")\n",
    "        metrics[metric].append(test_metric)\n",
    "    if metrics[\"discovery\"][-1] < best_sig:\n",
    "        best_params = params\n",
    "        best_sig = metrics[\"discovery\"][-1]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random visualisations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_grid = (jnp.array(list(range(1, num_steps + 1))) * batch_size) / train[0].shape[0]\n",
    "for k, v in metrics.items():\n",
    "    if k != \"generalised_variance\":\n",
    "        plt.plot(epoch_grid, v, label=k)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"metric\")\n",
    "plt.savefig(\"float.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields = neos.hists_from_nn(\n",
    "    best_params[\"nn_pars\"],\n",
    "    {k: v for k, v in zip(data_types, test)},\n",
    "    nn,\n",
    "    bandwidth=1e-8,\n",
    "    scale_factors={k: 2.0 if k == \"sig\" else 10.0 for k in data_types},\n",
    "    bins=jnp.array([0, *best_params[\"bins\"], 1]),\n",
    ")\n",
    "for c, (l, a) in zip(\n",
    "    [\"C0\", \"C1\", \"C2\", \"C3\"], zip(yields, jnp.array(list(yields.values())))\n",
    "):\n",
    "    plt.bar(range(len(a)), a, label=l, alpha=0.4, fill=None, edgecolor=c, linewidth=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network contours (only relevant for the blobs problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_space(\n",
    "    ax: Any,\n",
    "    params: dict[str, Any],\n",
    "    data: Array,\n",
    "    nn: Callable,\n",
    "    bins: Array | None = None,\n",
    ") -> None:\n",
    "    network = params[\"nn_pars\"]\n",
    "    bins = params[\"bins\"] if \"bins\" in params else bins\n",
    "    g = jnp.mgrid[-10:10:101j, -10:10:101j]\n",
    "    levels = [0, *bins, 1]\n",
    "    ax.contourf(\n",
    "        g[0],\n",
    "        g[1],\n",
    "        nn(network, jnp.moveaxis(g, 0, -1)).reshape(101, 101, 1)[:, :, 0],\n",
    "        levels=levels,\n",
    "        cmap=\"binary\",\n",
    "    )\n",
    "    ax.contour(\n",
    "        g[0],\n",
    "        g[1],\n",
    "        nn(network, jnp.moveaxis(g, 0, -1)).reshape(101, 101, 1)[:, :, 0],\n",
    "        colors=\"w\",\n",
    "        levels=levels,\n",
    "    )\n",
    "    sig, bkg_nom, bkg_up, bkg_down = data\n",
    "    # should definitely not have to repeat this every time lmao\n",
    "    ax.scatter(sig[:, 0], sig[:, 1], alpha=0.3, c=\"C9\", label=\"signal\")\n",
    "    ax.scatter(\n",
    "        bkg_up[:, 0],\n",
    "        bkg_up[:, 1],\n",
    "        alpha=0.1,\n",
    "        c=\"orangered\",\n",
    "        marker=6,\n",
    "        label=\"bkg up\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        bkg_down[:, 0],\n",
    "        bkg_down[:, 1],\n",
    "        alpha=0.1,\n",
    "        c=\"gold\",\n",
    "        marker=7,\n",
    "        label=\"bkg down\",\n",
    "    )\n",
    "    ax.scatter(bkg_nom[:, 0], bkg_nom[:, 1], alpha=0.3, c=\"C1\", label=\"bkg\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_data_space(ax, best_params, data=test, nn=nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d6333b89854cd01c2018f3ca2f5a59a2cde2765fbca789ff36cfad48ca629b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
